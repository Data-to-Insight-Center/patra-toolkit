{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1106a2d67b43cd4",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "\n",
    "# Getting Started with Patra Model Card Toolkit\n",
    "\n",
    "</div>\n",
    "\n",
    "The Patra Toolkit is a component of the Patra ModelCards framework designed to simplify the process of creating and documenting AI/ML models. It provides a structured schema that guides users in providing essential information about their models, including details about the model's purpose, development process, and performance. The toolkit also includes features for semi-automating the capture of key information, such as fairness and explainability metrics, through integrated analysis tools. By reducing the manual effort involved in creating model cards, the Patra Toolkit encourages researchers and developers to adopt best practices for documenting their models, ultimately contributing to greater transparency and accountability in AI/ML development.\n",
    "\n",
    "The Patra Toolkit embeds transparency and governance directly into the training workflow. Integrated scanners collect essential metadata—data sources, fairness metrics, and explainability insights—during model training and then generate a machine‑actionable JSON model card. These cards plug into the Patra Knowledge Base for rich queries on provenance, version history, and auditing. Flexible back‑ends publish models and artifacts to repositories such as Hugging Face or GitHub, automatically recording lineage links to trace every model’s evolution.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "This Colab Notebook is a quickstart guide that helps you:\n",
    "- Load and preprocess an example image (from an online URL)\n",
    "- Perform image classification with a pretrained ResNet50 model from PyTorch\n",
    "- Generate a comprehensive Model Card using the Patra Toolkit\n",
    "\n",
    "By the end of this tutorial, you will have a Model Card (in JSON format) that captures key metadata about your model and its predictions.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b125ea15282c07e0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-08T18:55:16.414923Z",
     "start_time": "2025-06-08T18:55:15.722131Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install torch torchvision patra_toolkit Pillow scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7650ae0f61ab8434",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-08T18:55:20.337727Z",
     "start_time": "2025-06-08T18:55:16.526554Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/neeleshkarthikeyan/d2i/patra-toolkit/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "\n",
    "from patra_toolkit import ModelCard, AIModel\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec80886749709316",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Load and Preprocess Image\n",
    "\n",
    "We'll download an example image from a URL. Then, we'll apply the same preprocessing as required by ResNet50:\n",
    "- Resize to 256 pixels on the smaller side\n",
    "- Center-crop to 224×224\n",
    "- Convert to a tensor and normalize using the ImageNet statistics\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af3a58fa90925977",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-08T18:55:20.429364Z",
     "start_time": "2025-06-08T18:55:20.358097Z"
    }
   },
   "outputs": [],
   "source": [
    "image_path = \"data/camera_trap_img.JPG\"\n",
    "image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "from torchvision import transforms\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "input_tensor = preprocess(image)\n",
    "input_batch = input_tensor.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ff12dc72138de9",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Model Training and Inference\n",
    "\n",
    "We load the pretrained ResNet50 model and perform inference on the preprocessed image. We'll then decode the top prediction using the default weights.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aafa2802bc26a67e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-08T18:55:20.934381Z",
     "start_time": "2025-06-08T18:55:20.450755Z"
    }
   },
   "outputs": [],
   "source": [
    "weights = torchvision.models.ResNet50_Weights.DEFAULT\n",
    "model = torchvision.models.resnet50(weights=weights)\n",
    "model.eval()\n",
    "\n",
    "# Perform inference\n",
    "with torch.no_grad():\n",
    "    output = model(input_batch)\n",
    "\n",
    "probabilities = F.softmax(output[0], dim=0)\n",
    "top5_prob, top5_catid = torch.topk(probabilities, 5)\n",
    "categories = weights.meta[\"categories\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f3716e9b40850ef",
   "metadata": {},
   "source": [
    "## 4. Building a Patra Model Card\n",
    "\n",
    "### 4.1 Basic Model Card Setup\n",
    "We start with essential metadata like name, version, short description, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a8632f1c190f585",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-08T18:55:20.954912Z",
     "start_time": "2025-06-08T18:55:20.952080Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create a ModelCard instance\n",
    "mc = ModelCard(\n",
    "    name=\"ResNet50\",\n",
    "    version=\"1.0\",\n",
    "    short_description=\"A pretrained ResNet50 image classifier\",\n",
    "    full_description=\"This model card demonstrates using a pretrained ResNet50 model from PyTorch\",\n",
    "    keywords=\"resnet50, pytorch, image classification, patra, pretrained\",\n",
    "    author=\"0009-0009-9817-7042\",\n",
    "    input_type=\"Image\",\n",
    "    category=\"classification\",\n",
    "    foundational_model=\"None\",\n",
    "    citation=\"https://doi.org/10.48550/arXiv.1512.03385\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a667fffe41a1b8",
   "metadata": {},
   "source": [
    "### 4.2 Attach AI Model Information\n",
    "Here we describe the model's ownership, license, performance metrics, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6768768217d7f55d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-08T18:55:20.974140Z",
     "start_time": "2025-06-08T18:55:20.970926Z"
    }
   },
   "outputs": [],
   "source": [
    "ai_model = AIModel(\n",
    "    name=\"ResNet50\",\n",
    "    version=\"1.0\",\n",
    "    description=\"Pretrained ResNet50 model from torchvision for image classification.\",\n",
    "    owner=\"0009-0009-9817-7042\",\n",
    "    location=\"\",  # will be updated after model submission\n",
    "    license=\"BSD-3 Clause\",\n",
    "    framework=\"pytorch\",\n",
    "    model_type=\"cnn\",\n",
    "    test_accuracy=0.75\n",
    ")\n",
    "\n",
    "# Attach the AIModel to the ModelCard\n",
    "mc.ai_model = ai_model\n",
    "mc.populate_requirements()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2fffdebfcdcdff",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Submission\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "59e434882f6b8486",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-08T22:16:36.706759Z",
     "start_time": "2025-06-08T22:16:28.539939Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Model card validation successful.\n",
      "INFO:root:PID created: 0009-0009-9817-7042-resnet50-1.0\n",
      "INFO:root:Model serialized successfully.\n",
      "0009-0009-9817-7042-resnet50-1.0.pt: 100%|██████████| 103M/103M [00:07<00:00, 13.1MB/s] \n",
      "INFO:root:Model uploaded at: https://huggingface.co/patra-iu/0009-0009-9817-7042-resnet50-1.0/blob/main/0009-0009-9817-7042-resnet50-1.0.pt\n",
      "INFO:root:Model card created.\n",
      "INFO:root:Model card uploaded at: https://huggingface.co/patra-iu/0009-0009-9817-7042-resnet50-1.0/blob/main/model_card.json\n",
      "/Users/neeleshkarthikeyan/d2i/patra-toolkit/.venv/lib/python3.10/site-packages/huggingface_hub/hf_api.py:9664: UserWarning: Warnings while validating metadata in README.md:\n",
      "- empty or missing yaml metadata in repo card\n",
      "  warnings.warn(f\"Warnings while validating metadata in README.md:\\n{message}\")\n",
      "INFO:root:Inference labels uploaded at: https://huggingface.co/patra-iu/0009-0009-9817-7042-resnet50-1.0/blob/main/labels.txt\n",
      "camera_trap_img.JPG: 100%|██████████| 1.74M/1.74M [00:00<00:00, 7.89MB/s]\n",
      "INFO:root:Artifact 'data/camera_trap_img.JPG' uploaded at: https://huggingface.co/patra-iu/0009-0009-9817-7042-resnet50-1.0/blob/main/camera_trap_img.JPG\n",
      "INFO:root:Model Card submitted successfully.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'success'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mc.submit(patra_server_url=\"http://149.165.151.249:5002/\",\n",
    "          model=model,\n",
    "          file_format=\"pt\",\n",
    "          model_store=\"huggingface\",\n",
    "          inference_labels=\"data/labels.txt\",\n",
    "          artifacts=[\"data/camera_trap_img.JPG\"]\n",
    "          )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7be012-4918-4245-8c52-889008934a9d",
   "metadata": {},
   "source": [
    "\n",
    "**[Optional] Tapis Authentication:**\n",
    "Before submitting, ensure you have obtained a valid Tapis token using your TACC credentials. If you do not already have a TACC account, you can create one at [https://accounts.tacc.utexas.edu/begin](https://accounts.tacc.utexas.edu/begin). You can use the `authenticate()` method provided by the toolkit (or any other method) to obtain the token. When calling the submission methods, pass the token as the `tapis_token` parameter so that your request is authenticated by the Patra server. If Tapis authentication isn’t required for your scenario, you can set `tapis_token` to `None`.\n",
    "\n",
    "The `mc.submit(...)` method can do one or more of the following:\n",
    "1. **Submit only the card** (no model, no artifacts).\n",
    "2. **Include the trained model** (uploading to Hugging Face or GitHub).\n",
    "3. **Add artifacts** (such as data files, inference labels, or any additional resources)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f5f9794fd8abaa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-08T18:55:21.004228Z",
     "start_time": "2025-06-08T18:55:21.001899Z"
    }
   },
   "outputs": [],
   "source": [
    "# tapis_token = mc.authenticate(username=\"neelk\", password=\"****\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9663cc-c094-4f2b-95a1-6557bfdb0c92",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
